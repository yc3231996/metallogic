import streamlit as st
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model
from typing import Any
import traceback
from db import get_dbmanager
from db.metastore import MetadataStore
from langchain_core.messages import HumanMessage
from langchain_core.callbacks import BaseCallbackHandler
from langgraph.checkpoint.memory import MemorySaver
from langchain_community.tools import TavilySearchResults
import uuid
from dotenv import load_dotenv
import base64
import json
from e2b_code_interpreter import Sandbox
from langchain_core.messages import SystemMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from prompt import planning, dataquery
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from langgraph.config import get_stream_writer
import decimal
from datetime import date, datetime

load_dotenv()

# åˆå§‹åŒ–å…¨å±€å˜é‡ï¼Œç”¨äºç¼“å­˜æ¨¡å‹å’Œcheckpointer
@st.cache_resource
def get_model(model_name: str):
    """ç¼“å­˜å¹¶è¿”å›LLMæ¨¡å‹å®ä¾‹"""
    if model_name == "gpt-4o":
        return init_chat_model( model="gpt-4o", model_provider="openai", streaming=True)
    elif model_name == "gpt-4o-mini":
        return init_chat_model( model="gpt-4o-mini", model_provider="openai", streaming=True)
    elif model_name == "gpt-4.1":
        return init_chat_model( model="gpt-4.1", model_provider="openai", streaming=True)
    elif model_name == "deepseek-reasoner":
        return init_chat_model(model="deepseek-reasoner", model_provider="deepseek", temperature=1.0, streaming=True)
        # return ChatOpenAI(model="deepseek-reasoner", base_url="https://genaiapi.cloudsway.net/v1/ai/yvmSooRghzahEnas", api_key="OJH88G8VbD1crCZhYiM6")
    elif model_name == "gemini-2.0":
        return init_chat_model(model="gemini-2.0-flash-001", model_provider="google_vertexai", streaming=True)
    elif model_name == "gemini-2.5":
        return init_chat_model(model="gemini-2.5-pro-exp-03-25", model_provider="google_vertexai", streaming=True)
    else:
        raise ValueError(f"Unsupported model: {model_name}")    

@st.cache_resource
def get_checkpointer():
    """ç¼“å­˜å¹¶è¿”å›MemorySaverå®ä¾‹"""
    return MemorySaver()

# @st.cache_resource
# def get_team():
#     graph = StateGraph(PlanExecute)
#     graph.add_node("planner", plan_step)
#     graph.add_node("executor", execute_step)

#     graph.add_edge(START, "planner")
#     graph.add_edge("planner", "executor")
#     graph.add_edge("executor", END)
#     return graph.compile(checkpointer=get_checkpointer())

@st.cache_resource
def get_coordinator():
    search_tool = TavilySearchResults(max_results=5)
    tools = [data_query_agent, chart_generate_agent, search_tool]
    system_prompt = planning.COORDINATOR_PROMPT
    agent = create_react_agent(get_model("gpt-4.1"), tools, prompt=system_prompt, checkpointer=get_checkpointer())
    return agent;


def main():
    with st.sidebar:
        st.title("æ•°æ®åˆ†æåŠ©æ‰‹")
        st.markdown("---")
        st.write("è¿™æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ä½ åˆ†ææ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚")
        
        # æ·»åŠ æ¸…é™¤ä¼šè¯æŒ‰é’®
        if st.button("æ¸…é™¤ä¼šè¯"):
            st.session_state.messages = [
                {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨åˆ†ææ•°æ®ï¼Œæœå¯»èµ„æ–™ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨ï¼Ÿ"}
            ]
            st.session_state.thread_id = str(uuid.uuid4())
            if "structured_messages" in st.session_state:
                st.session_state.structured_messages = []
            st.rerun()

    st.title("å‘Šè¯‰æˆ‘æ‚¨çš„éœ€æ±‚")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨åˆ†ææ•°æ®ï¼Œæœå¯»èµ„æ–™ï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨ï¼Ÿ"}
        ]
    
    # åˆå§‹åŒ–thread_idï¼Œç”¨äºè·Ÿè¸ªä¼šè¯
    if "thread_id" not in st.session_state:
        st.session_state["thread_id"] = str(uuid.uuid4())
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"], unsafe_allow_html=True)  # å…è®¸HTMLæ¸²æŸ“

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input(placeholder="è¾“å…¥æ‚¨çš„éœ€æ±‚..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # è·å–graph
        graph = get_coordinator()
        
        # åªåœ¨ç¬¬ä¸€æ¬¡äº¤äº’æ—¶ä¼ é€’å®Œæ•´å†å²ï¼ˆåŒ…æ‹¬åˆå§‹åŠ©æ‰‹æ¶ˆæ¯å’Œç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ï¼‰ï¼Œä¹‹ååªä¼ é€’æœ€æ–°æ¶ˆæ¯
        if len(st.session_state.messages) <= 2:
            agent_messages = [(msg["role"], msg["content"]) for msg in st.session_state.messages]
        else:
            # åªä¼ é€’æœ€æ–°æ¶ˆæ¯
            agent_messages = [("user", prompt)]
        
        # ä½¿ç”¨callbackå¤„ç†agentçš„å“åº”
        with st.chat_message("assistant"):
            config = {
                "configurable": {"thread_id": st.session_state.thread_id}
            }
            message_placeholder = st.empty()
            output = ""

            # ä½¿ç”¨stream_mode=["messages", "custom"]ï¼Œå¯ä»¥åŒæ—¶è·å–messageså’Œcustomçš„æµå¼è¾“å‡º
            for stream_mode, chunk in graph.stream({"messages": agent_messages}, config, stream_mode=["messages", "custom"]):
                if stream_mode == "messages": 
                    # chunk is tuple of AIMessage
                    if isinstance(chunk, tuple):
                        output += ''.join(item.content for item in chunk if hasattr(item, "content"))
                    else:
                        output += str(chunk)
                elif stream_mode == "custom":
                    output += str(chunk)
                message_placeholder.markdown(output, unsafe_allow_html=True)

            # # use messages stream mode
            # for chunk, metadata in graph.stream({"messages": agent_messages}, config, stream_mode="messages"):
            #     output += str(chunk.content)
            #     message_placeholder.markdown(output, unsafe_allow_html=True)

            st.session_state.messages.append({"role": "assistant", "content": output})



class PlanExecute(TypedDict):
    input: str
    plan: str
    result: str


# Each tool function can take a config argument. 
# In order for the config to be correctly propagated to the function, you MUST always add a RunnableConfig type annotation for your config argument
@tool
def data_query_agent(question: str, config: RunnableConfig) -> Any:
    """æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®å¹¶è¿”å›æŸ¥è¯¢ç»“æœã€‚
    
    è¯¥å·¥å…·ä¼šå°†é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢å¹¶æ‰§è¡Œã€‚
    
    Args:
        question: ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦æä¾›SQLï¼Œåªéœ€è¦æå‡ºéœ€æ±‚ã€‚
    
    Returns:
        æŸ¥è¯¢ç»“æœï¼Œé€šå¸¸æ˜¯JSONæ ¼å¼çš„æ•°æ®
    """
    try:
        writer = get_stream_writer()
        writer(f"<div style='font-size: 0.9em; color: #666; font-style: italic;'> ğŸ”§ <b>Agentå¼€å§‹æ‰§è¡Œ: data_query_agent </b> <p>è¾“å…¥: {question} </p> ")

        workspace = "yingkou-dw" # default workspace
        dbtype = "bigquery"
        metadata = MetadataStore(workspace).query()
        system_prompt = dataquery.SQL_PROMPT.format(workspace=workspace, metadata=metadata, dbtype=dbtype)
        tools = [execute_sql_query]
        messages = [HumanMessage(question)]

        agent = create_react_agent(get_model("gpt-4.1"), tools, prompt=system_prompt, name="data_query_agent")
        result = agent.invoke({"messages": messages}, config=config)

        writer(f" ğŸ”§ <b>data_query_agent æ‰§è¡Œå®Œæ¯•</b> </div>")
        # print(f"\n\n data_query_agent result: {result} \n\n")
        # TODO, return strucuted message?
        return result["messages"][-1].content
    except Exception as e:
        # error_msg = f"data query failed: {str(e)}"
        stack_trace = traceback.format_exc()
        error_msg = f"æ•°æ®æŸ¥è¯¢å¤±è´¥ (data_query_agent): \né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\nå †æ ˆè·Ÿè¸ª:\n{stack_trace}"
        print(f"\n\n===== DATA QUERY AGENT ERROR =====\n{error_msg}\n===========================\n\n")
        writer(f"<div style='color: red; background: #ffeeee; padding: 10px; border-radius: 5px; margin: 10px 0;'><b>æŸ¥è¯¢å‡ºé”™</b>: {str(e)}</div>")
        return f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}---{stack_trace}"


@tool
def chart_generate_agent(request: str, config: RunnableConfig, data: str=None) -> Any:
    """ç”Ÿæˆå›¾è¡¨å¹¶è¿”å›HTMLå›¾åƒã€‚
    
    è¯¥å·¥å…·ä¼šæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆå›¾è¡¨ã€‚æ•°æ®å¯ä»¥ç›´æ¥åœ¨requestä¸­æä¾›ï¼Œä¹Ÿå¯ä»¥åœ¨dataå‚æ•°ä¸­
    å¦‚æœæ•°æ®é‡ä¸å¤§ï¼Œå»ºè®®ç›´æ¥åœ¨requestå‚æ•°ä¸­åŒ…å«æ•°æ®
    
    Args:
        request: ç”¨æˆ·å…³äºå›¾è¡¨çš„é—®é¢˜
        data: å¯é€‰ã€‚ç”¨äºå¤„ç†çš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€‚
    
    Returns:
        HTMLå›¾åƒ
    """
    try:
        # TODO, upload data file to e2b sandbox
        writer = get_stream_writer()
        writer(f"<div style='font-size: 0.9em; color: #666; font-style: italic;'> ğŸ”§ <b>Agentå¼€å§‹æ‰§è¡Œ: chart_generate_agent </b> <p>è¾“å…¥: {request}; æ•°æ®: {data} </p> ")

        system_prompt = dataquery.CHART_PROMPT
        tools = [run_code]
        content = f"ç”¨æˆ·çš„é—®é¢˜: {request}\n æ•°æ®: {data}\n"
        print(f"\n\n chart_generate_agentï¼Œ user content: {content} \n\n")
        messages = [HumanMessage(content)]
        agent = create_react_agent(get_model("gpt-4.1"), tools, prompt=system_prompt, name="chart_generate_agent")
        result = agent.invoke({"messages": messages}, config=config)

        writer(f" ğŸ”§ <b>chart_generate_agent æ‰§è¡Œå®Œæ¯•</b> </div>")
        # TODO, return structured message?
        return result["messages"][-1].content
    except Exception as e:
        # error_msg = f"chart generate failed: {str(e)}"
        stack_trace = traceback.format_exc()
        error_msg = f"å›¾è¡¨ç”Ÿæˆå¤±è´¥ (chart_generate_agent): \né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\nå †æ ˆè·Ÿè¸ª:\n{stack_trace}"
        print(f"\n\n===== CHART GENERATE AGENT ERROR =====\n{error_msg}\n===========================\n\n")
        writer(f"<div style='color: red; background: #ffeeee; padding: 10px; border-radius: 5px; margin: 10px 0;'><b>å›¾è¡¨ç”Ÿæˆå‡ºé”™</b>: {str(e)}</div>")
        return f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}---{stack_trace}"



# åˆ›å»ºè‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†ç‰¹æ®Šç±»å‹
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        # å¤„ç†Decimalç±»å‹
        if isinstance(obj, decimal.Decimal):
            return float(obj)
        # å¤„ç†Dateç±»å‹
        elif isinstance(obj, date) and not isinstance(obj, datetime):
            return obj.isoformat()
        # å¤„ç†DateTimeç±»å‹
        elif isinstance(obj, datetime):
            return obj.isoformat()
        # å¤„ç†bytesç±»å‹
        elif isinstance(obj, bytes):
            return obj.decode('utf-8', errors='replace')
        # å¤„ç†é›†åˆç±»å‹
        elif isinstance(obj, set):
            return list(obj)
        # å¤„ç†å…¶ä»–ä¸å¯åºåˆ—åŒ–ç±»å‹
        try:
            return str(obj)
        except:
            return f"ä¸å¯åºåˆ—åŒ–å¯¹è±¡: {type(obj).__name__}"
        # å…¶ä»–ç±»å‹ä½¿ç”¨é»˜è®¤ç¼–ç å™¨å¤„ç†
        return super(CustomJSONEncoder, self).default(obj)

@tool
def execute_sql_query(workspace: str, sql: str) -> Any:
    """æ‰§è¡Œç»™å®šSQLè¯­å¥å¹¶è¿”å›æŸ¥è¯¢ç»“æœã€‚

    Args:
        workspace: å·¥ä½œåŒºåç§°
        sql: è¦æ‰§è¡Œçš„SQLè¯­å¥
    
    Returns:
        JSONæ ¼å¼çš„SQLæŸ¥è¯¢ç»“æœ
    """
    try:
        print("\n\n SQL to execute: ", sql)
        writer = get_stream_writer()
        writer(f"<div style='font-size: 0.9em; color: #666; font-style: italic;'> ğŸ”§ <b>Toolå¼€å§‹æ‰§è¡Œ: execute_sql_query </b>")
        
        dbmanger = get_dbmanager(workspace)
        query_result = dbmanger.execute_query(sql)

        writer(f" ğŸ”§ <b>execute_sql_query æ‰§è¡Œç»“æŸ </b> </div>")

        # ä½¿ç”¨è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†ç‰¹æ®Šç±»å‹
        print(f"\n\n sql execute result: {query_result} \n\n")
        
        # å°è¯•ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨è¿›è¡ŒJSONåºåˆ—åŒ–
        try:
            return json.dumps(query_result, cls=CustomJSONEncoder)
        except Exception as json_err:
            print(f"JSONåºåˆ—åŒ–å¤±è´¥: {str(json_err)}")
            # é€€å›åˆ°æ–‡æœ¬è¡¨ç¤º
            return f"æŸ¥è¯¢ç»“æœ (æ–‡æœ¬æ ¼å¼):\n{str(query_result)}"
            
    except Exception as e:
        stack_trace = traceback.format_exc()
        error_msg = f"SQLæ‰§è¡Œå¤±è´¥: \né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\nå †æ ˆè·Ÿè¸ª:\n{stack_trace}"
        print(f"\n\n===== SQL EXECUTION ERROR =====\n{error_msg}\n===========================\n\n")
        writer(f"<div style='color: red; background: #ffeeee; padding: 10px; border-radius: 5px; margin: 10px 0;'><b>SQLæ‰§è¡Œå‡ºé”™</b>: {str(e)}</div>")
        return f"<details><summary style='color: red;'><b>SQLæ‰§è¡Œå¤±è´¥</b>: {str(e)}</summary><pre style='background: #f8f8f8; padding: 10px; overflow: auto;'><code>{sql}</code>\n\n{stack_trace}</pre></details>"

@tool
def run_code(code: str) -> any:
    """ä½¿ç”¨E2Bæ²™ç®±æ‰§è¡ŒPythonä»£ç ï¼Œæ”¯æŒæ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€‚
    
    Args:
        code: è¦æ‰§è¡Œçš„Pythonä»£ç ï¼Œåº”å½“æ˜¯å®Œæ•´ä¸”è‡ªåŒ…å«çš„ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„å¯¼å…¥è¯­å¥ã€‚
             å¯ä»¥ç›´æ¥ä½¿ç”¨çš„ç±»åº“ï¼š
             - jupyter
             - numpy
             - pandas
             - matplotlib
             - seaborn
    Returns:
        ä»£ç æ‰§è¡Œçš„ç»“æœï¼ŒåŒ…æ‹¬è¾“å‡ºã€é”™è¯¯ä¿¡æ¯å’Œç”Ÿæˆçš„å›¾è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    try:
        writer = get_stream_writer()
        writer(f"<div style='font-size: 0.9em; color: #666; font-style: italic;'> ğŸ”§ <b>Toolå¼€å§‹æ‰§è¡Œ: run_code </b>")

        # è·å–æ²™ç®±å®ä¾‹
        sandbox = Sandbox()
        print("sandbox create, code to execute: ", code)    
        
        # æ‰§è¡Œä»£ç 
        execution = sandbox.run_code(code)
        
        # æ‰“å°å®Œæ•´çš„ç»“æœç»“æ„
        print('Code execution finished!')
        print(f'Results: {execution.results}')

        # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åŒ…å«PNGå›¾åƒ, ä¿å­˜å›¾ç‰‡åˆ°æ–‡ä»¶
        first_result = execution.results[0]
        if first_result.png:
        # Save the png to a file. The png is in base64 format.
            with open('./tmp/chart.png', 'wb') as f:
                f.write(base64.b64decode(first_result.png))
            print('Chart saved as chart.png')
            
        html_image = f"""
        <div>
            <img src="data:image/png;base64,{first_result.png}" style="max-width: 100%;">
        </div>
        """

        writer(f" ğŸ”§ <b>run_code æ‰§è¡Œç»“æŸ </b> </div>")
        return html_image
    except Exception as e:
        stack_trace = traceback.format_exc()
        error_msg = f"ä»£ç æ‰§è¡Œå¤±è´¥: \né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\nå †æ ˆè·Ÿè¸ª:\n{stack_trace}"
        print(f"\n\n===== CODE EXECUTION ERROR =====\n{error_msg}\n===========================\n\n")
        writer(f"<div style='color: red; background: #ffeeee; padding: 10px; border-radius: 5px; margin: 10px 0;'><b>ä»£ç æ‰§è¡Œå‡ºé”™</b>: {str(e)}</div>")
        
        code_with_line_numbers = "\n".join([f"{i+1}: {line}" for i, line in enumerate(code.split("\n"))])
        return f"""
        <details>
            <summary style='color: red;'><b>ä»£ç æ‰§è¡Œå¤±è´¥</b>: {str(e)}</summary>
            <div style='background: #f8f8f8; padding: 10px; overflow: auto; margin-top: 10px;'>
                <h4>æ‰§è¡Œçš„ä»£ç :</h4>
                <pre><code>{code_with_line_numbers}</code></pre>
                <h4>é”™è¯¯å †æ ˆ:</h4>
                <pre>{stack_trace}</pre>
            </div>
        </details>
        """



if __name__ == "__main__":
    DEBUG = False
    main()

